{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJJd5ib2Pcqv"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import pickle\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_aapl = pd.read_csv(\"AAPL.csv\")\n",
        "df_googl = pd.read_csv(\"GOOGL.csv\")\n",
        "df_meta = pd.read_csv(\"META.csv\")\n",
        "df_nvda = pd.read_csv(\"NVDA.csv\")\n",
        "df_tsla = pd.read_csv(\"TSLA.csv\")\n",
        "df_aapl[\"Date\"] = pd.to_datetime(df_aapl[\"Date\"], format=\"%Y-%m-%d\")\n",
        "df_googl[\"Date\"] = pd.to_datetime(df_googl[\"Date\"], format=\"%Y-%m-%d\")\n",
        "df_meta[\"Date\"] = pd.to_datetime(df_meta[\"Date\"], format=\"%Y-%m-%d\")\n",
        "df_nvda[\"Date\"] = pd.to_datetime(df_nvda[\"Date\"], format=\"%Y-%m-%d\")\n",
        "df_tsla[\"Date\"] = pd.to_datetime(df_tsla[\"Date\"], format=\"%Y-%m-%d\")\n",
        "df_aapl[\"Volume\"] = df_aapl[\"Volume\"].astype(float)\n",
        "df_googl[\"Volume\"] = df_googl[\"Volume\"].astype(float)\n",
        "df_meta[\"Volume\"] = df_meta[\"Volume\"].astype(float)\n",
        "df_nvda[\"Volume\"] = df_nvda[\"Volume\"].astype(float)\n",
        "df_tsla[\"Volume\"] = df_tsla[\"Volume\"].astype(float)\n",
        "df = pd.merge(df_aapl, df_googl, on=\"Date\", suffixes=(\"\", \"_googl\"))\n",
        "df = pd.merge(df, df_meta, on=\"Date\", suffixes=(\"\", \"_meta\"))\n",
        "df = pd.merge(df, df_nvda, on=\"Date\", suffixes=(\"\", \"_nvda\"))\n",
        "df = pd.merge(df, df_tsla, on=\"Date\", suffixes=(\"_aapl\", \"_tsla\"))"
      ],
      "metadata": {
        "id": "n-EtxNBQPoOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Q9fF7JrE-L1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "split = df[\"Date\"][(len(df[\"Date\"])*15)//20]\n",
        "names = df.columns\n",
        "y_names = [\"Date\"]\n",
        "for val in names:\n",
        "    if val[0] == \"O\" or val[0] == \"C\":\n",
        "        y_names.append(val)\n",
        "no_date = names[1:]\n",
        "no_date_y = y_names[1:]\n",
        "X = df.copy()\n",
        "y = df.copy()[y_names]\n",
        "X_train = pd.DataFrame(columns=X.columns)\n",
        "X_test = pd.DataFrame(columns=X.columns)\n",
        "X_test[\"X_date\"] = pd.Series(dtype=X[\"Date\"].dtype)\n",
        "X_train = X[:100]\n",
        "X_train[\"X_date\"] = X[\"Date\"][99]\n",
        "prev_val = X[\"Date\"][99]\n",
        "for val in X[\"Date\"][100:-10]:\n",
        "    if val < split:\n",
        "        X_train_new = X_train[X_train[\"X_date\"] == prev_val][1:]\n",
        "        X_train_new = pd.concat([X_train_new, X[X[\"Date\"] == val]])\n",
        "        X_train_new[\"X_date\"] = val\n",
        "        X_train = pd.concat([X_train, X_train_new])\n",
        "        prev_val = val\n",
        "    elif val == split:\n",
        "        X_test = X_train[X_train[\"X_date\"] == prev_val][1:]\n",
        "        X_test = pd.concat([X_test, X[X[\"Date\"] == val]])\n",
        "        X_test[\"X_date\"] = val\n",
        "        prev_val = val\n",
        "    else:\n",
        "        X_test_new = X_test[X_test[\"X_date\"] == prev_val][1:]\n",
        "        X_test_new = pd.concat([X_test_new, X[X[\"Date\"] == val]])\n",
        "        X_test_new[\"X_date\"] = val\n",
        "        X_test = pd.concat([X_test, X_test_new])\n",
        "        prev_val = val\n",
        "X_train = X_train.groupby(\"X_date\").agg(lambda x: list(x))\n",
        "X_test = X_test.groupby(\"X_date\").agg(lambda x: list(x))\n",
        "y_train = pd.DataFrame(columns=y.columns)\n",
        "y_train[\"X_date\"] = pd.Series(dtype=y[\"Date\"].dtype)\n",
        "y_test = pd.DataFrame(columns=y.columns)\n",
        "y_test[\"X_date\"] = pd.Series(dtype=y[\"Date\"].dtype)\n",
        "y_train = y[100:110]\n",
        "y_train[\"X_date\"] = y[\"Date\"][99]\n",
        "prev_val = y[\"Date\"][99]\n",
        "for val in y[\"Date\"][100:-10]:\n",
        "    if val < split:\n",
        "        y_train_new = y_train[y_train[\"X_date\"] == prev_val][1:]\n",
        "        y_train_new = pd.concat([y_train_new, y[y[\"Date\"] == val]])\n",
        "        y_train_new[\"X_date\"] = val\n",
        "        y_train = pd.concat([y_train, y_train_new])\n",
        "        prev_val = val\n",
        "    elif val == split:\n",
        "        y_test = y_train[y_train[\"X_date\"] == prev_val][1:]\n",
        "        y_test = pd.concat([y_test, y[y[\"Date\"] == val]])\n",
        "        y_test[\"X_date\"] = val\n",
        "        prev_val = val\n",
        "    else:\n",
        "        y_test_new = y_test[y_test[\"X_date\"] == prev_val][1:]\n",
        "        y_test_new = pd.concat([y_test_new, y[y[\"Date\"] == val]])\n",
        "        y_test_new[\"X_date\"] = val\n",
        "        y_test = pd.concat([y_test, y_test_new])\n",
        "        prev_val = val\n",
        "y_train = y_train.groupby(\"X_date\").agg(lambda x: list(x))\n",
        "y_test = y_test.groupby(\"X_date\").agg(lambda x: list(x))"
      ],
      "metadata": {
        "id": "LjL1I0mJPtDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_regressor(n_estimators=1000, max_depth=15, eta=0.1):\n",
        "    return MultiOutputRegressor(xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=n_estimators, max_depth=max_depth, eta=eta))\n",
        "\n",
        "def fit_regress(X_train, y_train, model):\n",
        "    y_train_used = np.array(y_train[:][no_date_y].values.tolist())\n",
        "    y_train_used = y_train_used.reshape((len(y_train_used), 100))\n",
        "    X_train_used = np.array(X_train[:][no_date].values.tolist())\n",
        "    X_train_used = X_train_used.reshape((len(X_train_used), 2500))\n",
        "    return model.fit(X_train_used, y_train_used)\n",
        "\n",
        "def predict_regress(X_test, model):\n",
        "    X_test_used = np.array(X_test[:][no_date].values.tolist())\n",
        "    X_test_used = X_test_used.reshape((len(X_test_used), 2500))\n",
        "    return model.predict(X_test_used)\n",
        "\n",
        "def evaluate_regress(X_test, y_test, model):\n",
        "    y_test_used = np.array(y_test[:][no_date_y].values.tolist())\n",
        "    y_test_used = y_test_used.reshape((len(y_test_used), 100))\n",
        "    return mean_squared_error(predict_regress(X_test, model), y_test_used)\n",
        "\n",
        "def regressor_grid(n_estimators_lst=[1000], max_depth_lst=[15], eta_lst=[0.1]):\n",
        "    models = []\n",
        "    histories = []\n",
        "    losses = []\n",
        "    min_loss = float(\"inf\")\n",
        "    min_loss_index = -1\n",
        "    min_loss_params = [None, None, None]\n",
        "    i = 0\n",
        "    for num in n_estimators_lst:\n",
        "        for depth in max_depth_lst:\n",
        "            for eta in eta_lst:\n",
        "                models.append(create_regressor(num, depth, eta))\n",
        "                histories.append(fit_regress(X_train, y_train, models[-1]))\n",
        "                loss = evaluate_regress(X_test, y_test, models[-1])\n",
        "                losses.append(loss)\n",
        "                if loss < min_loss:\n",
        "                    min_loss = loss\n",
        "                    min_loss_index = i\n",
        "                    min_loss_params = [num, depth, eta]\n",
        "                i += 1\n",
        "    return (models, histories, losses, min_loss_index, min_loss_params)"
      ],
      "metadata": {
        "id": "tXAeNiEPPkkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model_3 = create_regressor(10, 10, 0.3)\n",
        "#test_model_2 = create_regressor(10, 15, 0.3)\n",
        "#test_model_1 = create_regressor(10, 25, 0.3)\n",
        "#fit_regress(X_train, y_train, test_model_1)\n",
        "#print(evaluate_regress(X_test, y_test, test_model_1))\n",
        "#fit_regress(X_train, y_train, test_model_2)\n",
        "#print(evaluate_regress(X_test, y_test, test_model_2))\n",
        "fit_regress(X_train, y_train, test_model_3)\n",
        "print(evaluate_regress(X_test, y_test, test_model_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2jVfwykPzA7",
        "outputId": "e9b01925-93be-4a17-d649-da4dc9d2de34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2878.05342516996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "values = regressor_grid([3, 5, 10], [10, 15, 25], [0.1, 0.3, 0.05])"
      ],
      "metadata": {
        "id": "pcrjw7ykVv_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNayc2vNcin0",
        "outputId": "bffc4d59-002d-46cb-fa76-b69a57d9974a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15947.209929872071,\n",
              " 6898.812230559389,\n",
              " 20158.77015285705,\n",
              " 15947.209931262936,\n",
              " 6898.812231871514,\n",
              " 20158.770152855133,\n",
              " 15947.209931262936,\n",
              " 6898.812231871514,\n",
              " 20158.770152855133,\n",
              " 12120.9194109406,\n",
              " 4356.256219727695,\n",
              " 17349.516995156162,\n",
              " 12120.919411743054,\n",
              " 4356.256220956113,\n",
              " 17349.51699552873,\n",
              " 12120.919411743054,\n",
              " 4356.256220956113,\n",
              " 17349.51699552873,\n",
              " 6971.415566102075,\n",
              " 2878.05342516996,\n",
              " 12305.137767710143,\n",
              " 6971.415567624663,\n",
              " 2878.040173232087,\n",
              " 12305.13776783725,\n",
              " 6971.415567624663,\n",
              " 2878.03884951701,\n",
              " 12305.13776783725]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with open('model_2.pkl', 'wb') as f:\n",
        "  #pickle.dump(test_model_1, f)\n",
        "\n",
        "#with open('model_3.pkl', 'wb') as f:\n",
        "  #pickle.dump(test_model_2, f)\n",
        "\n",
        "with open('model_4.pkl', 'wb') as f:\n",
        "  pickle.dump(test_model_3, f)"
      ],
      "metadata": {
        "id": "nU8xZ08PdVdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_scaler = MinMaxScaler()\n",
        "y_scaler = MinMaxScaler()\n",
        "\n",
        "y_train_used = np.array(y_train[:][no_date_y].values.tolist())\n",
        "y_train_used = y_train_used.reshape((len(y_train_used), 100))\n",
        "X_train_used = np.array(X_train[:][no_date].values.tolist())\n",
        "X_train_used = X_train_used.reshape((len(X_train_used), 2500))\n",
        "y_test_used = np.array(y_test[:][no_date_y].values.tolist())\n",
        "y_test_used = y_test_used.reshape((len(y_test_used), 100))\n",
        "X_test_used = np.array(X_test[:][no_date].values.tolist())\n",
        "X_test_used = X_test_used.reshape((len(X_test_used), 2500))\n",
        "\n",
        "X_scaler.fit_transform(X_train_used)\n",
        "y_scaler.fit_transform(y_train_used)\n",
        "\n",
        "X_scaled = X_scaler.transform(X_train_used)\n",
        "y_scaled = y_scaler.transform(y_train_used)\n",
        "\n",
        "X_test_scaled = X_scaler.transform(X_test_used)\n",
        "y_test_scaled = y_scaler.transform(y_test_used)\n",
        "\n",
        "def fit_regress_scaled(X_train_scaled, y_train_scaled, model):\n",
        "    return model.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "def predict_regress_scaled(X_test_scaled, model):\n",
        "    return model.predict(X_test_scaled)\n",
        "\n",
        "def evaluate_regress_scaled(X_test_scaled, y_test_scaled, model):\n",
        "    return mean_squared_error(predict_regress_scaled(X_test_scaled, model), y_test_scaled)\n",
        "\n",
        "\n",
        "def regressor_grid_scaled(n_estimators_lst=[1000], max_depth_lst=[15], eta_lst=[0.1]):\n",
        "    models = []\n",
        "    histories = []\n",
        "    losses = []\n",
        "    min_loss = float(\"inf\")\n",
        "    min_loss_index = -1\n",
        "    min_loss_params = [None, None, None]\n",
        "    i = 0\n",
        "    for num in n_estimators_lst:\n",
        "        for depth in max_depth_lst:\n",
        "            for eta in eta_lst:\n",
        "                models.append(create_regressor(num, depth, eta))\n",
        "                histories.append(fit_regress_scaled(X_scaled, y_scaled, models[-1]))\n",
        "                loss = evaluate_regress_scaled(X_test_scaled, y_test_scaled, models[-1])\n",
        "                losses.append(loss)\n",
        "                if loss < min_loss:\n",
        "                    min_loss = loss\n",
        "                    min_loss_index = i\n",
        "                    min_loss_params = [num, depth, eta]\n",
        "                i += 1\n",
        "                print(i)\n",
        "    return (models, histories, losses, min_loss_index, min_loss_params)\n",
        "\n",
        "def inverse_scaler_evaluate(model, X_test_scaled, y_test):\n",
        "    return mean_squared_error(y_scaler.inverse_transform(predict_regress_scaled(X_test_scaled, model)), y_test_used)"
      ],
      "metadata": {
        "id": "Icjhr6LH9qG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values_2 = regressor_grid_scaled([15], [25, 50], [0.3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvYYmMO7jIHS",
        "outputId": "ce0c075a-651c-433e-ab27-8597c2fc46eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inverse_scaler_evaluate(values_2[0][0], X_test_scaled, y_test_used))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki1N6gam1w57",
        "outputId": "5aa29c7b-fa3d-4e17-860b-fc219629be39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2679.6141846507603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model_5.pkl', 'wb') as f:\n",
        "  pickle.dump(values_2[0][0], f)"
      ],
      "metadata": {
        "id": "PK92Bv-43IX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HfDlbA1pahgm"
      }
    }
  ]
}