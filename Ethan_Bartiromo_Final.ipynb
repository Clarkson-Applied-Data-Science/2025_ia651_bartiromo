{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b85377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cff6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aapl = pd.read_csv(\"AAPL.csv\")\n",
    "df_googl = pd.read_csv(\"GOOGL.csv\")\n",
    "df_meta = pd.read_csv(\"META.csv\")\n",
    "df_nvda = pd.read_csv(\"NVDA.csv\")\n",
    "df_tsla = pd.read_csv(\"TSLA.csv\")\n",
    "df_aapl[\"Date\"] = pd.to_datetime(df_aapl[\"Date\"], format=\"%Y-%m-%d\")\n",
    "df_googl[\"Date\"] = pd.to_datetime(df_googl[\"Date\"], format=\"%Y-%m-%d\")\n",
    "df_meta[\"Date\"] = pd.to_datetime(df_meta[\"Date\"], format=\"%Y-%m-%d\")\n",
    "df_nvda[\"Date\"] = pd.to_datetime(df_nvda[\"Date\"], format=\"%Y-%m-%d\")\n",
    "df_tsla[\"Date\"] = pd.to_datetime(df_tsla[\"Date\"], format=\"%Y-%m-%d\")\n",
    "df_aapl[\"Volume\"] = df_aapl[\"Volume\"].astype(float)\n",
    "df_googl[\"Volume\"] = df_googl[\"Volume\"].astype(float)\n",
    "df_meta[\"Volume\"] = df_meta[\"Volume\"].astype(float)\n",
    "df_nvda[\"Volume\"] = df_nvda[\"Volume\"].astype(float)\n",
    "df_tsla[\"Volume\"] = df_tsla[\"Volume\"].astype(float)\n",
    "df = pd.merge(df_aapl, df_googl, on=\"Date\", suffixes=(\"\", \"_googl\"))\n",
    "df = pd.merge(df, df_meta, on=\"Date\", suffixes=(\"\", \"_meta\"))\n",
    "df = pd.merge(df, df_nvda, on=\"Date\", suffixes=(\"\", \"_nvda\"))\n",
    "df = pd.merge(df, df_tsla, on=\"Date\", suffixes=(\"_aapl\", \"_tsla\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b83ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "split = df[\"Date\"][(len(df[\"Date\"])*15)//20]\n",
    "names = df.columns\n",
    "y_names = [\"Date\"]\n",
    "for val in names:\n",
    "    if val[0] == \"O\" or val[0] == \"C\":\n",
    "        y_names.append(val)\n",
    "no_date = names[1:]\n",
    "no_date_y = y_names[1:]\n",
    "X = df.copy()\n",
    "y = df.copy()[y_names]\n",
    "X_train = pd.DataFrame(columns=X.columns)\n",
    "X_test = pd.DataFrame(columns=X.columns)\n",
    "X_test[\"X_date\"] = pd.Series(dtype=X[\"Date\"].dtype)\n",
    "X_train = X[:100]\n",
    "X_train[\"X_date\"] = X[\"Date\"][99]\n",
    "prev_val = X[\"Date\"][99]\n",
    "for val in X[\"Date\"][100:-10]:\n",
    "    if val < split:\n",
    "        X_train_new = X_train[X_train[\"X_date\"] == prev_val][1:]\n",
    "        X_train_new = pd.concat([X_train_new, X[X[\"Date\"] == val]])\n",
    "        X_train_new[\"X_date\"] = val\n",
    "        X_train = pd.concat([X_train, X_train_new])\n",
    "        prev_val = val\n",
    "    elif val == split:\n",
    "        X_test = X_train[X_train[\"X_date\"] == prev_val][1:]\n",
    "        X_test = pd.concat([X_test, X[X[\"Date\"] == val]])\n",
    "        X_test[\"X_date\"] = val\n",
    "        prev_val = val\n",
    "    else:\n",
    "        X_test_new = X_test[X_test[\"X_date\"] == prev_val][1:]\n",
    "        X_test_new = pd.concat([X_test_new, X[X[\"Date\"] == val]])\n",
    "        X_test_new[\"X_date\"] = val\n",
    "        X_test = pd.concat([X_test, X_test_new])\n",
    "        prev_val = val\n",
    "X_train = X_train.groupby(\"X_date\").agg(lambda x: list(x))\n",
    "X_test = X_test.groupby(\"X_date\").agg(lambda x: list(x))\n",
    "y_train = pd.DataFrame(columns=y.columns)\n",
    "y_train[\"X_date\"] = pd.Series(dtype=y[\"Date\"].dtype)\n",
    "y_test = pd.DataFrame(columns=y.columns)\n",
    "y_test[\"X_date\"] = pd.Series(dtype=y[\"Date\"].dtype)\n",
    "y_train = y[100:110]\n",
    "y_train[\"X_date\"] = y[\"Date\"][99]\n",
    "prev_val = y[\"Date\"][99]\n",
    "for val in y[\"Date\"][100:-10]:\n",
    "    if val < split:\n",
    "        y_train_new = y_train[y_train[\"X_date\"] == prev_val][1:]\n",
    "        y_train_new = pd.concat([y_train_new, y[y[\"Date\"] == val]])\n",
    "        y_train_new[\"X_date\"] = val\n",
    "        y_train = pd.concat([y_train, y_train_new])\n",
    "        prev_val = val\n",
    "    elif val == split:\n",
    "        y_test = y_train[y_train[\"X_date\"] == prev_val][1:]\n",
    "        y_test = pd.concat([y_test, y[y[\"Date\"] == val]])\n",
    "        y_test[\"X_date\"] = val\n",
    "        prev_val = val\n",
    "    else:\n",
    "        y_test_new = y_test[y_test[\"X_date\"] == prev_val][1:]\n",
    "        y_test_new = pd.concat([y_test_new, y[y[\"Date\"] == val]])\n",
    "        y_test_new[\"X_date\"] = val\n",
    "        y_test = pd.concat([y_test, y_test_new])\n",
    "        prev_val = val\n",
    "y_train = y_train.groupby(\"X_date\").agg(lambda x: list(x))\n",
    "y_test = y_test.groupby(\"X_date\").agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c2e2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seed, num_neurons, drop_rate, activations):\n",
    "    tf.random.set_seed(seed)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(2500,)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    for i in range(min(len(num_neurons), len(activations))):\n",
    "        model.add(tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=0)))\n",
    "        model.add(tf.keras.layers.SimpleRNN(num_neurons[i],  activation=activations[i]))\n",
    "    if drop_rate > 0.0:\n",
    "        model.add(tf.keras.layers.Dropout(rate=drop_rate))\n",
    "    model.add(tf.keras.layers.Dense(100))\n",
    "    model.compile(optimizer=\"Adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def use_model(seed, num_neurons=[10], drop_rate=0.0, activations=[\"relu\"]):\n",
    "    model = create_model(seed, num_neurons, drop_rate, activations)\n",
    "    history = fit_mod(X_train, y_train, model)\n",
    "    loss = evaluate_mod(X_test, y_test, model)\n",
    "    return (model, history, loss)\n",
    "\n",
    "def fit_mod(X_train, y_train, model):\n",
    "    y_train_used = np.array(y_train[:][no_date_y].values.tolist())\n",
    "    y_train_used = y_train_used.reshape((len(y_train_used), 100))\n",
    "    X_train_used = np.array(X_train[:][no_date].values.tolist())\n",
    "    X_train_used = X_train_used.reshape((len(X_train_used), 2500))\n",
    "    return model.fit(X_train_used, y_train_used, epochs=100, callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=15)])\n",
    "\n",
    "def predict_mod(X_test, model):\n",
    "    X_test_used = np.array(X_test[:][no_date].values.tolist())\n",
    "    X_test_used = X_test_used.reshape((len(X_test_used), 2500))\n",
    "    return model.predict(X_test_used, batch_size=1)\n",
    "\n",
    "def evaluate_mod(X_test, y_test, model):\n",
    "    y_test_used = np.array(y_test[:][no_date_y].values.tolist())\n",
    "    y_test_used = y_test_used.reshape((len(y_test_used), 100))\n",
    "    return mean_squared_error(predict_mod(X_test, model), y_test_used) \n",
    "\n",
    "\n",
    "def grid_search(num_seeds=2, num_neurons_lst = [[10]], drop_rate_lst = [0.0], activations_lst = [[\"relu\"]]):\n",
    "    models = []\n",
    "    histories = []\n",
    "    losses = []\n",
    "    min_loss = float(\"inf\")\n",
    "    min_loss_index = -1\n",
    "    min_loss_params = [None, None, None, None]\n",
    "    i = 0\n",
    "    np.random.seed = 42\n",
    "    for i in range(num_seeds):\n",
    "        seed = np.random.randint(1,100)\n",
    "        for num_neurons in num_neurons_lst:  \n",
    "            for activations in activations_lst:\n",
    "                if len(activations) == len(num_neurons):\n",
    "                    for drop_rate in drop_rate_lst:\n",
    "                        model, history, loss = use_model(seed, num_neurons, drop_rate, activations)\n",
    "                        models.append(model)\n",
    "                        histories.append(history)\n",
    "                        losses.append(loss)\n",
    "                        if loss < min_loss:\n",
    "                            min_loss = loss\n",
    "                            min_loss_index = i\n",
    "                            min_loss_params = [seed, num_neurons, drop_rate, activations]\n",
    "                        i += 1\n",
    "    return (models, histories, losses, min_loss_index, min_loss_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14d4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regressor(n_estimators=10, max_depth=15, eta=0.1):\n",
    "    return MultiOutputRegressor(xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=n_estimators, max_depth=max_depth, eta=eta))\n",
    "\n",
    "def fit_regress(X_train, y_train, model):\n",
    "    y_train_used = np.array(y_train[:][no_date_y].values.tolist())\n",
    "    y_train_used = y_train_used.reshape((len(y_train_used), 100))\n",
    "    X_train_used = np.array(X_train[:][no_date].values.tolist())\n",
    "    X_train_used = X_train_used.reshape((len(X_train_used), 2500))\n",
    "    return model.fit(X_train_used, y_train_used)\n",
    "\n",
    "def predict_regress(X_test, model):\n",
    "    X_test_used = np.array(X_test[:][no_date].values.tolist())\n",
    "    X_test_used = X_test_used.reshape((len(X_test_used), 2500))\n",
    "    return model.predict(X_test_used)\n",
    "\n",
    "def evaluate_regress(X_test, y_test, model):\n",
    "    y_test_used = np.array(y_test[:][no_date_y].values.tolist())\n",
    "    y_test_used = y_test_used.reshape((len(y_test_used), 100))\n",
    "    return mean_squared_error(predict_regress(X_test, model), y_test_used) \n",
    "\n",
    "def regressor_grid(n_estimators_lst=[1000], max_depth_lst=[15], eta_lst=[0.1]):\n",
    "    models = []\n",
    "    histories = []\n",
    "    losses = []\n",
    "    min_loss = float(\"inf\")\n",
    "    min_loss_index = -1\n",
    "    min_loss_params = [None, None, None]\n",
    "    i = 0\n",
    "    for num in n_estimators_lst:\n",
    "        for depth in max_depth_lst:\n",
    "            for eta in eta_lst:\n",
    "                models.append(create_regressor(num, depth, eta))\n",
    "                histories.append(fit_regress(X_train, y_train, models[-1]))\n",
    "                loss = evaluate_regress(X_test, y_test, models[-1])\n",
    "                losses.append(loss)\n",
    "                if loss < min_loss:\n",
    "                    min_loss = loss\n",
    "                    min_loss_index = i\n",
    "                    min_loss_params = [num, depth, eta]\n",
    "                i += 1\n",
    "    return (models, histories, losses, min_loss_index, min_loss_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4706a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "y_train_used = np.array(y_train[:][no_date_y].values.tolist())\n",
    "y_train_used = y_train_used.reshape((len(y_train_used), 100))\n",
    "X_train_used = np.array(X_train[:][no_date].values.tolist())\n",
    "X_train_used = X_train_used.reshape((len(X_train_used), 2500))\n",
    "y_test_used = np.array(y_test[:][no_date_y].values.tolist())\n",
    "y_test_used = y_test_used.reshape((len(y_test_used), 100))\n",
    "X_test_used = np.array(X_test[:][no_date].values.tolist())\n",
    "X_test_used = X_test_used.reshape((len(X_test_used), 2500))\n",
    "\n",
    "X_scaler.fit_transform(X_train_used)\n",
    "y_scaler.fit_transform(y_train_used)\n",
    "\n",
    "X_scaled = X_scaler.transform(X_train_used)\n",
    "y_scaled = y_scaler.transform(y_train_used)\n",
    "\n",
    "X_test_scaled = X_scaler.transform(X_test_used)\n",
    "y_test_scaled = y_scaler.transform(y_test_used)\n",
    "\n",
    "def fit_regress_scaled(X_train_scaled, y_train_scaled, model):\n",
    "    return model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "def predict_regress_scaled(X_test_scaled, model):\n",
    "    return model.predict(X_test_scaled)\n",
    "\n",
    "def evaluate_regress_scaled(X_test_scaled, y_test_scaled, model):\n",
    "    return mean_squared_error(predict_regress_scaled(X_test_scaled, model), y_test_scaled)\n",
    "\n",
    "\n",
    "def regressor_grid_scaled(n_estimators_lst=[1000], max_depth_lst=[15], eta_lst=[0.1]):\n",
    "    models = []\n",
    "    histories = []\n",
    "    losses = []\n",
    "    min_loss = float(\"inf\")\n",
    "    min_loss_index = -1\n",
    "    min_loss_params = [None, None, None]\n",
    "    i = 0\n",
    "    for num in n_estimators_lst:\n",
    "        for depth in max_depth_lst:\n",
    "            for eta in eta_lst:\n",
    "                models.append(create_regressor(num, depth, eta))\n",
    "                histories.append(fit_regress_scaled(X_scaled, y_scaled, models[-1]))\n",
    "                loss = evaluate_regress_scaled(X_test_scaled, y_test_scaled, models[-1])\n",
    "                losses.append(loss)\n",
    "                if loss < min_loss:\n",
    "                    min_loss = loss\n",
    "                    min_loss_index = i\n",
    "                    min_loss_params = [num, depth, eta]\n",
    "                i += 1\n",
    "                print(i)\n",
    "    return (models, histories, losses, min_loss_index, min_loss_params)\n",
    "\n",
    "def inverse_scaler_evaluate(model, X_test_scaled, y_test):\n",
    "    return mean_squared_error(y_scaler.inverse_transform(predict_regress_scaled(X_test_scaled, model)), y_test_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07df32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barti\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\barti\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\barti\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\barti\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\barti\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\barti\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\barti\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\barti\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\barti\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\barti\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "So there was an issue with loading the model, because apparently when loading the model from keras, it forgets that tensorflow exists altogether\n",
    "and so even with the proposed online solution, which works for everybody else, it will not perform any calculations on my model because it has a\n",
    "Lambda layer with a tensorflow function in it, and keras's load_model aspects just won't work with non builtin functions, so I would need to run\n",
    "the code again, which for some reason even with the same seed does not give the same model, so the best RNN model is virtually lost forever,\n",
    "however it was not the best model of them all anyway, so I'm glad I was able to beat it with an XGBoost model.\n",
    "\"\"\"\n",
    "\n",
    "#tf.keras.config.enable_unsafe_deserialization()\n",
    "#Best_RNN_Model= tf.keras.models.load_model(\"my_model_1.keras\", custom_objects={'tf': tf})\n",
    "#RNN_error = evaluate_mod(X_test, y_test, Best_RNN_Model)\n",
    "RNN_error = 9352.20\n",
    "xgboost_model_1, xgboost_model_2, xgboost_model_3, xgboost_model_4 = (None, None, None, None)\n",
    "with open(\"model_2.pkl\", \"rb\") as f:\n",
    "    xgboost_model_1 = pkl.load(f)\n",
    "    xg1_error = evaluate_regress(X_test, y_test, xgboost_model_1)\n",
    "with open(\"model_3.pkl\", \"rb\") as f:\n",
    "    xgboost_model_2 = pkl.load(f)\n",
    "    xg2_error = evaluate_regress(X_test, y_test, xgboost_model_2)\n",
    "with open(\"model_4.pkl\", \"rb\") as f:\n",
    "    xgboost_model_3 = pkl.load(f)\n",
    "    xg3_error = evaluate_regress(X_test, y_test, xgboost_model_3)\n",
    "with open(\"model_5.pkl\", \"rb\") as f:\n",
    "    xgboost_model_4 = pkl.load(f)\n",
    "    xg4_error = inverse_scaler_evaluate(xgboost_model_4, X_test_scaled, y_test_scaled)\n",
    "y_model = y[y[\"Date\"] < split][no_date_y].apply(lambda x: ExponentialSmoothing(x, seasonal_periods=250, seasonal=\"additive\", trend=\"additive\", damped_trend=True))\n",
    "y_fitted = []\n",
    "y_forecasted = []\n",
    "for val in y_model:\n",
    "    y_fitted.append(val.fit())\n",
    "    y_forecasted.append(y_fitted[-1].forecast(steps=len(y[y[\"Date\"] >= split])))\n",
    "    #print(y_forecasted)\n",
    "error = sum(mean_squared_error(y_forecasted[i], y[y[\"Date\"] >= split][y.columns[i + 1]]) for i in range(10))/10\n",
    "plt.figure(figsize=[20, 20])\n",
    "plt.bar([\"Recurrent Neural Network\", \"XG Boost 10-depth\", \"XG Boost 15-depth\", \"XG Boost 25-depth\", \"XG Boost scaled\" , \"Exponential Smoothing\"], [RNN_error, xg1_error, xg2_error, xg3_error, xg4_error, error])\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "I also just realized that in my previous analysis when doing the exponential smoothing I was using the wrong value for the MSE\n",
    "I was accidentally adding the MSE of the 10 variables together, but forgetting to divide by 10, after correcting this, it is much more reasonable\n",
    "and is actually slightly better than the RNN that I made anyway.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11e473",
   "metadata": {},
   "source": [
    "This analysis shows that the best model for five of the most popular tech stock prices is the Scaled XG Boost algorithm on 15 estimators, maximum depth of 25, with a learning rate of 0.3.  I believe that if I used 1000 estimators it could have been much much better, however, since it takes approximately 1.5 to 5 minutes to run each estimator depending on the hyperparameters, then it could be said to run for at least 25 hours straight, for which I do not have the cpu power, nor the google colab ability to run the code for that long to train the xg boost algorithm, in addition, with that many estimators, it could potentially overfit the training data, and be useless anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9817d86f",
   "metadata": {},
   "source": [
    "The analysis shows that the average dollar amount wrong in the prediction is approximately $50 off the correct price, based on MSE.  Also, the way I predicted was by using the previous 100 days of data from all 5 variables inside each of the 5 datasets, and tried to predict specifically the Open and Close values for the following 14 days on the 5 datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
